# ğŸ‘ï¸ Gaze Tracking System

A hybrid **Gaze Tracking System** that captures and analyzes human eye gaze using **OpenCV** and **MediaPipe**.  
It supports both **manual point calibration** and **automatic calibration**, combining **head pose** and **eye gaze** data for accurate gaze estimation.

---

## ğŸš€ Features

- **Real-time gaze tracking** with MediaPipe Face Mesh.
- **Head pose integration** for enhanced accuracy.
- **Eye gaze estimation** using iris and eye landmarks.
- **9-point calibration** (blink-based or OK button-based).
- **Screen boundary mapping** for precise screen coordinates.
- **Normal smoothing** for stable gaze points.
- **Blink detection** for calibration triggers.
- **System monitoring** (CPU, RAM usage) integrated in logs.
- **Logging** in JSON, CSV, and TXT formats.
- **Visualization** overlay with gaze points, crosshair, calibration targets, and warnings.
- Adjustable **cursor sensitivity** for smoothing responsiveness.

---

## ğŸ§© System Architecture

```mermaid
flowchart TD
    A[Camera Input] --> B[MediaPipe Face Mesh]
    B --> C[Landmark Extraction]
    C --> D[Head Pose Estimation]
    C --> E[Eye Feature Extraction]
    C --> F[Iris Detection]
    D --> G[Head Gaze Vector]
    E --> H[Eye Gaze Vector]
    F --> H
    G --> I[Hybrid Fusion Engine]
    H --> I
    I --> J[Gaze Estimation Output]
    J --> K[Visualization and Logging]
```

---

## ğŸ› ï¸ Installation

### Step 1 â€“ Clone the Repository
```bash
git clone https://github.com/yourusername/gaze-tracking-system.git
cd gaze-tracking-system
```

### Step 2 â€“ Create Virtual Environment (Optional)
```bash
python -m venv venv
# Activate it
venv\Scripts\activate        # On Windows
source venv/bin/activate     # On macOS/Linux
```

### Step 3 â€“ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ“¦ Requirements

Your `requirements.txt` should contain:
```
opencv-python
mediapipe
numpy
```

**Optional (for data logging and analysis):**
```
pandas
matplotlib
scikit-learn
scipy
```

---

## â–¶ï¸ Usage

### Run the Gaze Tracker
```bash
python gaze_tracker.py
```

**###Calibration Tips (Critical for Accuracy!)**

Sit ~50â€“70 cm from camera
Good, even lighting on your face (avoid backlight)
Look directly at each yellow circle
Keep head as still as possible during calibration
In blink mode: one clear blink per point when ready
In OK mode: look + click the green "OK" button

### Keyboard Controls

| Key   | Function                               |
| ----- | ------------------------------------   |
| **q** | Quit                                   |
| **s** | Set screen boundaries                  |
| **c** | Start point calibration                |
| **r** | Reset calibration                      |
| **b** | Switch calibration to BLINK method     |
| **o** | Switch calibration to OK button method |
| **+** | Increase sensitivity                   |
| **-** | Decrease sensitivity                   |
| **l** | Toggle logging                         |


---

## ğŸ§  Working Principle

1. **Face Detection** â€” MediaPipe detects 3D facial landmarks and iris positions.  
2. **Head Pose Estimation** â€” Calculates head orientation using selected landmarks.  
3. **Eye Gaze Vector** â€” Derived from iris center and eye corners.  
4. **Hybrid Fusion** â€” Merges head and eye gaze for stable tracking.  
5. **Calibration** â€” Maps gaze direction to screen coordinates.  
6. **Visualization** â€” Displays tracking overlay and calibration points.  

---

## ğŸ—‚ï¸ Folder Structure

```
ğŸ“ gaze-tracking-system
â”‚
â”œâ”€â”€ gaze_tracker.py           # Main script
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                 # Documentation
â””â”€â”€ gaze_logs/                # auto-created logs (JSON, CSV, TXT)
    â”œâ”€â”€ gaze_data_20251120_185918.csv        
    â”œâ”€â”€ gaze_data_20251120_185918.json         
    â”œâ”€â”€ gaze_data_20251120_185918.txt  
```

---

## ğŸŒŸ Future Enhancements

- ğŸ”¥ Heatmap visualization for gaze concentration  
- ğŸ“ˆ Real-time analytics dashboard  
- ğŸ§© Deep learningâ€“based gaze refinement  
- ğŸ’» Web-based gaze tracking (WebGazer.js integration)  
- ğŸ¯ Eye-controlled UI navigation  

---

## ğŸ§‘â€ğŸ’» Author

**BalaJi**  

---

## ğŸ“„ License

Licensed under the **MIT License**.  
You may use, modify, and distribute this project with attribution.

---

## ğŸ™Œ Acknowledgements

- [MediaPipe by Google](https://developers.google.com/mediapipe)  
- [OpenCV](https://opencv.org/)  
- [WebGazer.js](https://webgazer.cs.brown.edu/) â€“ concept inspiration  

---

â­ **If you found this project useful, please give it a star on GitHub!**
